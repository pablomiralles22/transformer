{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "13b50caa-412d-4ce2-bfd6-5041c4decf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "81bb12cc-cc88-4aa1-80a6-d05f45b49ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logical_not(torch.ones((5, 5), dtype=bool).tril())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "259ebe2d-4eca-43a8-9677-56dff2962745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_self_attention(\n",
    "    Q,  # query (...BATCH, TARGET_SEQ_DIM, QK_DIM)\n",
    "    K,  # key   (...BATCH, SRC_SEQ_DIM, QK_DIM)\n",
    "    V,  # value (...BATCH, SRC_SEQ_DIM, V_DIM)\n",
    "    attn_mask=None, # (...BATCH, TARGET_SEQ_DIM, SRC_SEQ_DIM) of BOOL\n",
    "    is_causal=False,  # causal attention masking\n",
    "):  # -> (...BATCH, TARGET_SEQ_DIM, V_DIM)\n",
    "    *batch_dim, target_seq_dim, qk_dim = Q.shape\n",
    "    src_seq_dim = K.size(-2)\n",
    "    \n",
    "    K_T = K.transpose(-2, -1)\n",
    "    attn_logits = (Q @ K_T / (qk_dim ** 0.5))\n",
    "    \n",
    "    if attn_mask is None:\n",
    "        attn_mask = torch.ones((target_seq_dim, src_seq_dim), dtype=bool, requires_grad=False)\n",
    "    if is_causal is True:\n",
    "        attn_mask &= torch.ones((target_seq_dim, src_seq_dim), dtype=bool, requires_grad=False).tril()\n",
    "\n",
    "    attn_logits = attn_logits.masked_fill(torch.logical_not(attn_mask), -float('inf'))\n",
    "    attn_weight = torch.softmax(attn_logits, dim=-1)\n",
    "    result = attn_weight @ V\n",
    "    \n",
    "    return result, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4bfac194-a4fd-41f1-a327-40610c840bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6975, 0.4676, 0.4804, 0.7067, 0.6771, 0.9294, 0.8087],\n",
       "          [0.5592, 0.5975, 0.4835, 0.7846, 0.6701, 0.7906, 0.5447],\n",
       "          [0.5577, 0.5248, 0.3294, 0.6061, 0.7338, 0.6542, 0.5945],\n",
       "          [0.4969, 0.5615, 0.5238, 0.5219, 0.7216, 0.6420, 0.4770]],\n",
       " \n",
       "         [[0.1245, 0.5985, 0.9666, 0.4224, 0.0725, 0.8845, 0.6975],\n",
       "          [0.1245, 0.5985, 0.9666, 0.4224, 0.0725, 0.8845, 0.6975],\n",
       "          [0.2116, 0.5361, 0.6706, 0.5960, 0.3072, 0.8712, 0.4641],\n",
       "          [0.3834, 0.6951, 0.4935, 0.5209, 0.4729, 0.7073, 0.4580]]]),\n",
       " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5741, 0.4259, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3090, 0.3100, 0.3810, 0.0000, 0.0000],\n",
       "          [0.2505, 0.1797, 0.2708, 0.2990, 0.0000]],\n",
       " \n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5421, 0.0000, 0.4579, 0.0000, 0.0000],\n",
       "          [0.2533, 0.2228, 0.2694, 0.2545, 0.0000]]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.rand(2, 4, 6)\n",
    "K = torch.rand(2, 5, 6)\n",
    "V = torch.rand(2, 5, 7)\n",
    "attn_mask = torch.rand(2, 4, 5) < 0.9\n",
    "\n",
    "scaled_dot_product_self_attention(Q, K, V, attn_mask=attn_mask, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b4b77014-807d-47d0-9d3f-c9cb77a01d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        bias=True,\n",
    "        kfeatdim=None,\n",
    "        vfeatdim=None,\n",
    "        vdim=None,\n",
    "    ):\n",
    "        assert embed_dim % num_heads == 0, \"Error: the embedding dimension should be divisible by the number of heads\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.kfeatdim = embed_dim if kfeatdim is None else kfeatdim\n",
    "        self.vfeatdim = embed_dim if vfeatdim is None else vfeatdim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.vdim = self.head_dim if vdim is None else vdim\n",
    "        \n",
    "        self.WQ = nn.Linear(embed_dim, embed_dim, bias=bias)  # (num_heads, embed_dim, head_dim) ~ (embed_dim, embed_dim)\n",
    "        self.WK = nn.Linear(self.kfeatdim, embed_dim, bias=bias)  # (num_heads, kfeatdim, head_dim) ~ (kfeatdim, embed_dim)\n",
    "        self.WV = nn.Linear(self.vfeatdim, num_heads * self.vdim, bias=bias)  # (num_heads, vfeatdim, vdim) - (vfeatdim, num_heads * vdim)\n",
    "        self.WO = nn.Linear(num_heads * self.vdim, embed_dim, bias=bias)  # (num_heads * vdim, embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        Q,  # (...BATCH, TARGET_SEQ_DIM, embed_dim)\n",
    "        K,  # (...BATCH, SRC_SEQ_DIM, kfeatdim)\n",
    "        V,  # (...BATCH, SRC_SEQ_DIM, vfeatdim)\n",
    "        key_padding_mask=None,  # (...BATCH, SRC_SEQ_DIM)\n",
    "        attn_mask=None,  # (TARGET_SEQ_DIM, SRC_SEQ_DIM) or (...BATCH, num_heads, TARGET_SEQ_DIM, SRC_SEQ_DIM)\n",
    "        is_causal=False\n",
    "    ):  # -> (...BATCH, TARGET_SEQ_DIM, embed_dim) ; OPT attention weights\n",
    "        *batch_dim, target_seq_dim, _ = Q.shape\n",
    "        *_, src_seq_dim, _ = K.shape\n",
    "        \n",
    "        Q_proj = self.__separate_heads(self.WQ(Q))  # (...BATCH, num_heads, TARGET_SEQ_DIM, head_dim)\n",
    "        K_proj = self.__separate_heads(self.WK(K))  # (...BATCH, num_heads, SRC_SEQ_DIM, head_dim)\n",
    "        V_proj = self.__separate_heads(self.WV(V))  # (...BATCH, num_heads, SRC_SEQ_DIM, vdim)\n",
    "\n",
    "        # Padding to mask (...BATCH, SRC_SEQ_DIM) -> (...BATCH, 1, 1, SRC_SEQ_DIM)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.view(*batch_dim, 1, 1, src_seq_dim)\n",
    "            \n",
    "        attn_mask = self.__merge_masks(key_padding_mask, attn_mask)\n",
    "        \n",
    "        heads, attn_weigth = scaled_dot_product_self_attention(\n",
    "            Q_proj, K_proj, V_proj, attn_mask, is_causal\n",
    "        )  # (...BATCH, num_heads, TARGET_SEQ_DIM, vdim)\n",
    "        \n",
    "        return self.WO(self.__join_heads(heads)), attn_weigth\n",
    "\n",
    "    def __separate_heads(self, mat):\n",
    "        # (...BATCH, SEQ, num_heads * proj_dim) -> (...BATCH, num_heads, SEQ, proj_dim)\n",
    "        *batch_dim, seq_dim, proj_dim = mat.shape\n",
    "        return mat.view(*batch_dim, seq_dim, self.num_heads, proj_dim // self.num_heads).transpose(-2, -3)\n",
    "    \n",
    "    def __join_heads(self, mat):\n",
    "        # (...BATCH, num_heads, SEQ, proj_dim) -> (...BATCH, SEQ, num_heads * proj_dim)\n",
    "        *batch_dim, num_heads, seq_dim, proj_dim = mat.shape\n",
    "        return mat.transpose(-2, -3).contiguous().view(*batch_dim, seq_dim, num_heads * proj_dim)\n",
    "\n",
    "    def __merge_masks(self, *masks):\n",
    "        masks = [mask for mask in masks if mask is not None]\n",
    "        if len(masks) == 0: return None\n",
    "        final_mask = masks[0]\n",
    "        for mask in masks[1:]:\n",
    "            final_mask &= mask\n",
    "        return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d65d57ac-85e6-4c4c-a5e3-48deab1d1849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 16])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma = MultiheadAttention(16, 4)\n",
    "Q = torch.rand(2, 2, 4, 16)  # (...BATCH, TARGET_SEQ_DIM, embed_dim)\n",
    "K = torch.rand(2, 2, 3, 16) # (...BATCH, SRC_SEQ_DIM, kfeatdim)\n",
    "V = torch.rand(2, 2, 3, 16) # (...BATCH, SRC_SEQ_DIM, vfeatdim)\n",
    "ma.forward(Q, K, V)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2829f4f3-5f80-4f9f-9eb8-76c00087ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads=4,\n",
    "        p_dropout=0.1,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=nn.ReLU,\n",
    "        layer_norm_eps=1e-05,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mh_attention = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.dropout_1 = nn.Dropout(p_dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim, layer_norm_eps)\n",
    "        \n",
    "        self.ff_l1 = nn.Linear(embed_dim, dim_feedforward)\n",
    "        self.activation = activation()\n",
    "        self.ff_l2 = nn.Linear(dim_feedforward, embed_dim)\n",
    "        self.dropout_2 = nn.Dropout(p_dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim, layer_norm_eps)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src,  # (...BATCH, SRC_SEQ_DIM, embed_dim)\n",
    "        src_mask=None,  # (TARGET_SEQ_DIM, SRC_SEQ_DIM) or (...BATCH, num_heads, TARGET_SEQ_DIM, SRC_SEQ_DIM)\n",
    "        src_key_padding_mask=None,  # (...BATCH, SRC_SEQ_DIM)\n",
    "        is_causal=False\n",
    "    ):  # -> (...BATCH, TARGET_SEQ_DIM, embed_dim) ; OPT attention weights\n",
    "        *batch_dim, src_seq_dim, _ = src.shape\n",
    "\n",
    "        x, _ = self.mh_attention(src, src, src, src_key_padding_mask, src_mask, is_causal)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.layer_norm_1(src + x)\n",
    "        \n",
    "        y = self.ff_l2(self.activation(self.ff_l1(x)))\n",
    "        y = self.dropout_2(y)\n",
    "        return self.layer_norm_2(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "42d02cd0-0185-4fac-a3f7-96786cc8be66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5431, -1.0846, -0.9011,  0.8957,  0.5347, -0.6398,  0.1175,\n",
       "            1.9653, -1.0611,  0.5459, -2.0089,  0.7471,  0.4047,  0.8768,\n",
       "            0.2236, -1.1588],\n",
       "          [-0.8203, -0.1239, -0.7349, -0.8493,  1.3454,  1.2977,  0.4614,\n",
       "            1.2601, -1.6294,  0.1631, -0.4998, -0.1653,  1.9158, -0.2224,\n",
       "            0.0425, -1.4405],\n",
       "          [-0.3493,  0.0955,  0.6185, -1.9106,  1.4605,  0.5502,  0.8610,\n",
       "           -0.6984, -0.8787,  0.7809, -2.2249,  0.6605,  0.7090, -0.4498,\n",
       "            0.8332, -0.0577],\n",
       "          [-0.9797, -0.5216, -1.3075, -0.8019,  1.5519,  0.7661,  0.6680,\n",
       "            0.8565, -0.7424, -0.0350, -1.9466,  0.1383,  1.8502,  0.5921,\n",
       "           -0.2695,  0.1811]],\n",
       "\n",
       "         [[ 0.1245, -1.1639, -0.7406, -1.4156,  1.1140, -0.4898,  0.8720,\n",
       "            1.8175, -0.2387,  0.5932, -1.9118,  0.5538,  0.9475,  0.8126,\n",
       "           -0.1045, -0.7701],\n",
       "          [ 1.3201, -0.6396,  0.4988, -0.0741,  1.0548, -0.0987, -0.4132,\n",
       "            0.5036, -1.3795, -0.4137, -2.4190,  1.0091, -0.2238,  0.8763,\n",
       "            1.2495, -0.8507],\n",
       "          [-0.4949, -1.3116, -0.4451, -0.7860,  0.1020,  1.3798,  0.1067,\n",
       "            1.6013, -0.6537,  0.4332, -1.8964, -0.7310,  1.8456,  0.3751,\n",
       "            0.6400, -0.1650],\n",
       "          [ 0.4809, -0.2924,  0.6019, -1.8506,  0.5982,  0.1187,  0.3301,\n",
       "            1.2925, -1.7850,  0.4103, -1.8373,  0.0276,  1.2517,  0.9799,\n",
       "           -0.6289,  0.3022]]],\n",
       "\n",
       "\n",
       "        [[[-0.6108, -0.3005, -0.3513, -1.0787,  0.3411,  1.5279, -0.2003,\n",
       "            2.2265, -1.3351,  0.8254, -1.4087,  0.4970,  0.7185, -0.6683,\n",
       "            0.6592, -0.8419],\n",
       "          [-0.3976,  0.0324,  0.6234, -1.1559,  1.6983, -0.8425,  0.2706,\n",
       "            0.7064, -2.0487, -0.4146, -1.3050, -0.3187,  1.4212,  1.1543,\n",
       "            0.6472, -0.0708],\n",
       "          [ 0.7034, -1.3599,  0.0097, -0.4937,  0.7874, -0.5174,  0.6419,\n",
       "            0.4118, -1.7279,  0.8624, -1.6551,  1.4423, -0.0286,  1.3829,\n",
       "            0.5996, -1.0588],\n",
       "          [-0.4981, -0.9661,  0.6174, -0.9656,  1.1720,  0.6971,  1.1710,\n",
       "            1.4260, -1.7686,  0.3505, -1.5106, -0.0161,  1.4137, -0.0352,\n",
       "           -0.3023, -0.7850]],\n",
       "\n",
       "         [[-0.4833, -0.9125,  0.8583, -1.2445,  1.1625,  1.2096, -0.1176,\n",
       "            0.7871, -0.9104, -0.6306, -1.8356,  0.1529,  1.0832,  1.3958,\n",
       "            0.5570, -1.0719],\n",
       "          [ 0.1873, -1.4266,  0.7076, -0.9201,  0.0935,  1.1595,  1.0351,\n",
       "            1.4011, -0.8593, -0.4159, -1.3824, -0.7371,  0.6554,  1.5506,\n",
       "            0.3256, -1.3743],\n",
       "          [ 0.1183, -0.0957,  0.6679, -1.4125,  1.2858,  0.4069, -0.7031,\n",
       "            1.4966, -1.5236, -0.4393, -1.4669,  0.0497,  1.3298,  1.3881,\n",
       "           -0.4759, -0.6259],\n",
       "          [ 0.6591, -0.7204, -0.0421, -1.7913,  1.7597,  0.9908,  0.5750,\n",
       "            1.8330, -1.2383, -0.6433,  0.0100, -0.2408,  0.8150, -0.5285,\n",
       "           -0.5136, -0.9244]]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el = TransformerEncoderLayer(16)\n",
    "src = torch.rand(2, 2, 4, 16)  # (...BATCH, SRC_SEQ_DIM, embed_dim)\n",
    "el.forward(src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
